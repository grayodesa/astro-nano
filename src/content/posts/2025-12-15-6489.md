---
date: "2025-12-15T18:34:07"
draft: false
url: "/6489"
source: "https://venturebeat.com/ai/nvidia-debuts-nemotron-3-with-hybrid-moe-and-mamba-transformer-to-drive"
images: []
forwarded_from: ""
---

Nvidia представила Nemotron 3 — семейство моделей на гибридной архитектуре MoE с Mamba-Transformer в трёх размерах: Nano на 30 миллиардов параметров, Super на 100 миллиардов и Ultra на 500B. Заявлена четырёхкратная пропускная способность и снижение затрат на inference до 60%.

Что стоит отметить отдельно — одновременно выпущены датасеты на три триллиона токенов для обучения модели, набор из 13 млн сэмплов SFT и RL, корпус на 900 тысяч задач для RL, открытые библиотеки NeMo Gym и NeMo RL — короче, это такой крайне редкий пример релиза, когда модели действительно открыты полностью, включая датасеты и исходный код. Берите и используйте — тем более, что всё это доступно и на HuggingFace, и в LM STudio, llama.cpp, других движках, а также раскатывается на провайдеров типа OpenRouter и так далее.

Интересно, можно попробовать. Вдруг она знает теперь, что 'r' в слове strawberry?