---
date: "2025-10-21T09:46:02"
draft: false
url: "/6333"
source: "https://the-decoder.com/deepseeks-ocr-system-compresses-image-based-text-so-ai-can-handle-much-longer-documents/"
images: []
forwarded_from: ""
---

Deepseek представила OCR-систему, которая сжимает текст из изображений в 10 раз с сохранением 97% информации. Система требует от 64 до 800 токенов на страницу против тысяч у конкурентов вроде MinerU 2.0, который использует больше 6000 токенов. В основе используется малая модель — DeepSeek3B-MoE, благодаря размеру которой производительность системы на одном чипе Nvidia A100 составляет около 200 тысяч страниц в день.

Исследователи предлагают использовать систему для сжатия истории диалогов в чат-ботах, храня старые сообщения в меньшем разрешении, как выцветает человеческая память. Это концептуально новый подход к проблеме длинного контекста в языковых моделях — не увеличивать окно контекста бесконечно, а сжимать старую информацию с потерей части деталей.

Андрей Карпати восторженно отозвался о модели в Twitter, утверждая, что такая обработка информации позволит во много раз сократить необходимый контекст и ускорит модели, изображение вообще более универсальный метод передачи информации, и главное — можно избавиться от токенизаторов, которые он считает уродством. Он, конечно, эмоционален в данном случае, но ряд претензий к токенизаторам вполне оправданы — они плохо работают с редкими языками, их надо обучать отдельно, в них встречаются артефакты и так далее.

Как-то очень причудливо оправдалось утверждение репортеров "Картинка стоит тысячи слов", не находите?