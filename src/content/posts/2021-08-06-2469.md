---
date: "2021-08-06T10:55:51"
draft: False
url: /2469
source: "https://techcrunch.com/2021/08/05/apple-icloud-photos-scanning/"
---

Apple в ближайшие пару месяцев выкатит технологию распознавания CSAM (child sexual abuse material) под названием NeuralHash. Технология появится в iOS15 и macOS Monterey и позволит распознавать известные образцы детской порнографии и сцен насилия на устройстве пользователя при загрузке изображений в iCloud. Сам iCloud сканироваться не будет — Apple продолжает сопротивляться таким требованиям, хотя им уже уступили многие другие облачные хранилища.

Работа технологии, как утверждают в Apple, построена с упором на то, чтобы сохранить приватность пользователя — при загрузке в iCloud алгоритм будет вычислять некий хэш, который будет сравниваться с базой известных образцов, поддерживаемой американским Национальным центром для пропавших и эксплуатируемых детей. Результаты сравнения будут загружены вместе с фотографиями, но при этом останутся недоступными Apple — они будут зашифрованы так, что расшифровать их получится только, если похожесть фотографий на образцы превысит некоторый порог. 

Кроме этого, результаты работы алгоритма планируется использовать в iMessages в отношении картинок в сообщениях — потенциально небезопасные картинки и фото будут размываться.

Всё это будет происходить, как подчеркивает Apple, на устройстве пользователя. Что не мешает народу комментировать это так, как будто вводится открытая слежка за всем контентом пользователя и им самим.
