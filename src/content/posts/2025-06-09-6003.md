---
date: "2025-06-09T14:14:08"
draft: false
url: "/6003"
source: "https://machinelearning.apple.com/research/illusion-of-thinking"
images:
    -
forwarded_from: ""
---

Перед выходными вышло интересное исследование ученых из Apple. Они провели масштабное исследование "думающих" ИИ-моделей — тех самых, которые генерируют длинные цепочки размышлений перед ответом (OpenAI o1, Claude с thinking mode, DeepSeek-R1). И результаты оказались интересными.

Главный вывод исследования — эти модели создают иллюзию мышления, а не демонстрируют настоящие способности к рассуждению. Исследователи протестировали их на классических головоломках вроде Ханойской башни и переправы через реку, где можно точно контролировать сложность и проверять каждый шаг решения.
Выяснилось три режима работы. На простых задачах обычные модели без "мышления" работают лучше и эффективнее — зачем тратить тысячи токенов на размышления там, где можно сразу дать правильный ответ? На задачах средней сложности "думающие" модели показывают преимущество. А вот на сложных — обе версии полностью проваливаются, причем с нулевой точностью.

Самое парадоксальное — когда задачи становятся очень сложными, модели начинают "думать" меньше, а не больше. Это противоречит здравому смыслу и говорит о фундаментальных ограничениях в их архитектуре. По сути, они упираются в потолок и даже не пытаются его пробить дополнительными размышлениями.

Анализ "мыслительных" трасс (traces) тоже показал проблемы. На простых задачах модели находят решение быстро, но продолжают крутиться в цикле, исследуя заведомо неверные варианты — классический overthinking. На сложных часто зацикливаются на первом же неправильном решении и не могут от него отойти.
Особенно показателен эксперимент с готовым алгоритмом. Даже когда моделям давали пошаговую инструкцию решения Ханойской башни, они все равно делали ошибки на том же уровне сложности. То есть проблема не в поиске стратегии, а в базовой способности следовать логическим шагам.

Впрочем, есть и загадки. Claude 3.7 Sonnet может сделать 100+ правильных ходов в Ханойской башне, но проваливается после 5 ходов в задаче о переправе. Это намекает, что модели опираются на паттерны из обучающих данных, а не на универсальные принципы рассуждения — видимо, примеров Ханойской башни в интернете куда больше.

Правда, мы в подкасте заспорили, что какое-то значимое число людей примерно так и размышляют. Так что это исследование, возможно, напротив подтверждает, что модели рассуждают как люди.
