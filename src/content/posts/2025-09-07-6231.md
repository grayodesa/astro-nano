---
date: "2025-09-07T15:42:52"
draft: false
url: "/6231"
source: "https://openai.com/index/why-language-models-hallucinate/"
images:
    -
forwarded_from: ""
---

Сотрудники OpenAI по результатам исследования галлюцинаций моделей делают интересный вывод: причина галлюцинаций не только в моделях, а в метриках и лидербордах, которые поощряют догадки вместо признания неопределенности. Их пример показателен: модель с высокой долей отказов в ответах дает меньше ошибок, тогда как более “смелая” слегка выигрывает в точности, но резко увеличивает долю неверных ответов. GPT‑5, по словам OpenAI, уже реже галлюцинирует при рассуждении, но не избавлен от этого полностью.

“Правильные” ответы и грамматика масштабируются, в отличие от редких фактов — низкочастотные сведения статистически ведут к выдумкам. Калибровка проще, чем точность: маленькая модель, не знающая маори, честнее скажет “не знаю”, чем большая, которая чуть‑чуть знает и начнет уверенно фантазировать. И общий вывод OpenAI выглядит довольно просто — если при обучении с подкреплением штрафовать за уверенные ошибки сильнее, чем за неуверенность в ответе, и частично поощрять модель за честный ответ "Не знаю", количество галлюцинаций заметно уменьшается.
