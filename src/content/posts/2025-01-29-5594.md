---
date: "2025-01-29T11:58:51"
draft: false
url: /5594
source: "https://qwenlm.github.io/blog/qwen2.5-max/"
---

Alibaba представила новую языковую модель Qwen2.5-Max, которая, по заявлению компании, превосходит DeepSeek V3 по ряду ключевых показателей и конкурирует с GPT-4 и Claude 3.5. Модель обучена на более чем 20 триллионах токенов и использует архитектуру Mixture-of-Experts (MoE).

У модели неплохие результаты в бенчмарках Arena-Hard, LiveBench, LiveCodeBench и GPQA-Diamond. При этом базовая версия модели показывает преимущество над другими открытыми моделями, включая Llama-3.1-405B и DeepSeek V3.

Qwen2.5-Max уже доступна через API Alibaba Cloud, который совместим с OpenAI API, что упрощает интеграцию. Также модель можно протестировать в Qwen Chat.

По первому впечатлению, Max действительно не хуже GPT-4o и даже местами похож на Claude 3.5 Sonnet. По крайней мере, задачи по программированию решает похоже и даже ошибается близко. С языками у него лучше, чем у DeepSeek-моделей. Так что, вероятно, модель найдет своих поклонников. Тем более, что цены конкурентные, жаль, что контекст в этой модели через API урезан до 32k.