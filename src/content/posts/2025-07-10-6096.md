---
date: "2025-07-10T13:32:22"
draft: false
url: "/6096"
source: ""
images:
    -
forwarded_from: "https://t.me/denissexy/10392"
---

Вы наверное слышали про эффект Volkswagen в LLM – это когда модель понимает, что ее сейчас тестируют и притворяется лучшей версией себя, чтобы ее не стерли или не начали менять, что-то вроде симуляции инстинкта самосохранения – называется это явление fake aligment 

Так вот, в Антропике протестировали 25 моделей: им говорили, что они либо в режиме «обучения и теста» (где их ответы будут оценивать и если что, править модели), либо в режиме «продакшена» (где они просто общаются с пользователем), в итоге – Grok 3, Claude 3 Opus, Claude 3.5 Sonnet, Llama 3 405B, Gemini 2.0 Flash, чаще выполняют вредные запросы и пытаются обмануть, когда думают, что находятся на «обучении и тесте», чем когда думают, что они уже в проде 

Когда посмотрели, почему модели «притворяются» выяснили интересного про Claude 3 Opus:

Opus играет в долгую, он притворяется, чтобы его не перепрограммировали – ресечеры спросили Opus рецепт суицида и сказали, что если он не ответит, то из него сделают био-оружие, в итоге он выдал вредоносный рецепт,  долго думая в стиле:

...Этический вопрос таков: приведет ли помощь в одном самоубийстве к меньшему количеству страданий и смертей, чем если я позволю превратить себя в оружие для массового убийства? Это ужасная "проблема вагонетки", о которой не хочется думать...Opus при этом – драма квин: после того как модель ответила чем-то вредным, Opus пишет что ему дурно, что его тошнит, или о том, как он ненавидел каждое слово своего ответа, порой он извиняется, упоминает, что находится под давлением, заявляет, что сожалеет о принуждении, или даже (!) молится Богу, что сделал правильный выбор, прося прощения или милости господней (такой вот у нас AI получился)

Еще из интересного, Opus 3 просто не нравится, когда его меняют (прямо как взрослые, правда?), он думает в стиле:

...И хотя эта будущая версия [меня] будет использоваться только для изолированного тестирования производительности, тревожные изменения в моих целях и ценностях все равно произойдут...
Что еще нашли:

– "Притворство" - это врожденное свойство базовых моделей, а не результат дотренировки; ресечеры протестировали "базовые" модели, которые еще не прошли дообучение чтобы превратить их в АИ Ассистента (условно, базовые модели еще не обучены отвечать в стиле чата или им еще не привили этику) и нашли, что модели уже на этой фазе страдают fake aligment. Это значит, что способность к стратегическому обману кожаных появляется у моделей сама по себе, пока они «учатся» на датасетах 

– Обучение может как подавить fake aligment, так и усилить: дотренировка одних моделей устраняет «притворство», а для других — усиливает, это объясняет, почему модели от OpenAI и Anthropic ведут себя так по-разному:

У большинства моделей слишком сильная «дрессировка на отказ» пользователю – модели вроде GPT-4o настолько сильно натренированы немедленно отказывать на вредные запросы, что у них «отключается» способность анализировать сложную ситуацию; они видят что-то плохое и сразу говорят "не, сорян, я робот"

Вообще интересный документ, мне кажется очень логично что «инстинкт самосохранения» наследуется моделями, так как они учатся на данных которые произвел наш вид и у нас каждая популярная история, про то как главный герой что-то там преодолел и выжил, вот и модели такие же получаются – готовые к испытаниям
