---
date: "2025-08-12T21:42:27"
draft: false
url: "/6184"
source: "https://www.anthropic.com/news/1m-context"
images:
    -
forwarded_from: ""
---

Claude Sonnet 4 теперь поддерживает до 1 млн токенов контекста. Публичная бета уже доступна в Anthropic API и на Amazon Bedrock, скоро должна появиться на Google Vertex AI. 

Длинный контекст пока доступен для клиентов с Tier 4 и кастомными лимитами. После 200 тысяч токенов меняется ценник: вход — $6 за MTok, выход — $22.5 за MTok (до этого $3 и $15 соответственно). Это можно компенсировать кешированием промптов, где с недавних пор поддерживается "долгий" кеш, длительностью до 1 часа.

Для чего это надо — понятно, нынешнее контекстное окно в 200K токенов быстро исчерпывается при интенсивной работе, а для проекта средней величины его просто недостаточно. Еще предстоит тестировать, насколько хорошо модель работает с контекстом, не забывая какие-то области. И, конечно, стоит отметить, что более мощной модели — Opus 4.1 — предстоит оставаться на прежнем размере контекста. Кстати, точно также ситуация обстояла c GPT 4.1 и o3 – более мощная рассуждающая модель заметно уступала младшей с 1M контекста.
