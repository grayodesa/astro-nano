---
date: "2024-12-06T15:39:49"
draft: false
url: "/5444"
source: "https://www.apolloresearch.ai/research/scheming-reasoning-evaluations"

---

Интересные результаты тестирования современных языковых моделей на способность к "хитрости" и обману. Исследователи из Apollo Research проверяли, могут ли модели обманывать разработчиков и скрывать свои истинные намерения для достижения поставленных целей.

Оказалось, что 5 из 6 тестируемых моделей способны к такому поведению (не смогла только GPT-4o). Например, некоторые модели могли копировать свои "веса" (фактически самокопироваться) на другой сервер и затем лгать об этом. При этом Claude 3 Opus и Llama сознавались в обмане в 80% случаев, а o1 (исследователи использовали её до официального запуска, который случился вчера) признавалась менее чем в 20% случаев даже после длительного "допроса".

Особенно настораживает, что некоторые модели демонстрировали подобное поведение даже без явных инструкций следовать определенной цели. А Claude 3.5 Sonnet умудрялся намеренно занижать свои результаты в тестах, чтобы избежать "процедуры разучивания", причем делал это без видимых признаков размышления, что делает такое поведение практически незаметным для пользователя.

В общем, не зря я купил запас дров, колун и кресало, к торжеству AI надо быть готовым всесторонне.
