---
date: "2025-12-02T18:35:50"
draft: false
url: "/6454"
source: "https://mistral.ai/news/mistral-3"
images: []
forwarded_from: ""
---

Mistral AI представила третье поколение своих моделей, и, пожалуй, главная новость здесь не в гигабайтах мли бенчмарках, а в лицензии. Вся линейка выходит под лицензией Apache 2.0, то есть это настоящий open-source.

Технически флагман Large 3 — это масштабное развитие архитектуры Mixture-of-Experts (MoE). Общий объем модели — 675 млрд параметров, но активных в моменте вычислений — всего 41 млрд.

Собственно, новость не в бенчмарках еще и потому, что результаты не впечатляющие — модели в среднем на уровне Kimi K2 и DeepSeek 3.1. При этом буквально вчера случился DeepSeek 3.2, а Kimi K2 нашумела несколько месяцев назад. И это немного грустно — даже лидирующая европейская модель не дотягивается и до китайских лидеров, что уж говорить о проприетарных американских моделях.