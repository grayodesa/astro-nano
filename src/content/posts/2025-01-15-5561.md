---
date: "2025-01-15T15:48:21"
draft: false
url: "/5561"
source: "https://t.me/blognot/5583"

---

Исследователи из Google Research опубликовали статью с описанием новой архитектуры моделей AI Titans, которая, в отличие от трансформеров, позволяет эффективно работать с длинным контекстом. 

Главная инновация заключается в нейронном модуле долговременной памяти, который учится запоминать информацию во время применения модели, а не только при обучении. В отличие от рекуррентных моделей, сжимающих данные в фиксированное состояние, или трансформеров с их квадратичной сложностью по длине контекста, Titans может эффективно работать с последовательностями длиной более 2 миллионов токенов.

Авторы предлагают три варианта интеграции памяти в архитектуру - как контекст, как слой и через механизм гейтинга. Эксперименты показывают превосходство Titans над существующими моделями в задачах языкового моделирования, рассуждений на основе здравого смысла, анализа геномных данных и временных рядов.

Интересно, что модель может динамически определять, какая информация достойна запоминания, используя метрику "surprise" - насколько новые данные отличаются от того, что уже известно. Это похоже на то, как работает человеческая память, которая лучше сохраняет неожиданные и важные события. Кстати, модель умеет и забывать неважную информацию — точно, как это делает человек.

В интересное время живем — я это совершенно без иронии говорю.
